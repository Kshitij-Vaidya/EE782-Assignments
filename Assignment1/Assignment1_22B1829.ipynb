{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85a5a941",
   "metadata": {},
   "source": [
    "# Assignment 1 : End-to-End Image Captioning for Remote Sensing\n",
    "\n",
    "**Name : Kshitij Vaidya**\n",
    "\n",
    "**Roll Number : 22B1829**\n",
    "\n",
    "[Google Drive Video Submission](https://drive.google.com/drive/folders/1NkJjQ0YAYkjuV6vtw_bc4hxnU4twDCrx?usp=drive_link)\n",
    "\n",
    "[GitHub Repository](https://github.com/Kshitij-Vaidya/EE782-Assignments)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4036c127",
   "metadata": {
    "vscode": {
     "languageId": "latex"
    }
   },
   "source": [
    "## Abstract\n",
    "\n",
    "This report presents the design, implementation and evaluation of two deep learning architectures for the task of remote sensing image captioning. The models are trained and evaluated on the **Remote Sensing Image Captioning Dataset (RSICD)**. I implemented two prominent encoder-decoder frameworks: a Convolutional Neural Network (CNN) paired with a Long Short-Term Memory (LSTM) network and a CNN combined with a Transformer Decoder. Shared across both model, **ResNet-18** and **MobileNet** backbones pre-trained on ImageNet act as the visual feature extractors. Performance is assessed using the BLEU-4 and METEOR metrics and qualitatively through analysis of success and failure on specific data slices. We also explore model explainability using techniques like **Grad-CAM** for visual saliency and Attention Map analysis for textual importance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50722040",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Image captioning, the task of automatically generating a textual description for an image, stands at the intersection of computer vision and natural language processing. It requires a model to not only identify objects within an image but also to understand their relationships and articulate them in a coherent, human-like sentence.\n",
    "\n",
    "While general image captioning is a well-studied problem, its application to remote sensing and overhead imagery presents unique challenges. Unlike typical ground-level photographs, satellite images often feature different scales, overhead perspectives, and specific land-use patterns (e.g., farmlands, harbors, runways) that demand specialized understanding. Furthermore, the orientation of objects in aerial imagery can be arbitrary, introducing a need for rotational invariance.\n",
    "\n",
    "This assignment aims to build two end-to-end captioning models from scratch for the RSICD dataset. Specifically, implement and evaluate a classic CNN-LSTM architecture and a more modern CNN-Transformer decoder architecture. The goal is to compare these two popular approaches in the context of remote sensing data. A shared CNN encoder, utilizing either a ResNet-18 or MobileNet backbone, extracts visual features that are then passed to the respective language decoders to generate descriptive captions.\n",
    "\n",
    "Beyond standard performance metrics like BLEU-4 and METEOR, this report emphasizes a comprehensive analysis of model behavior. This includes a qualitative review of generated captions across different data slices—such as high versus low contrast images or scenes depicting specific geographic features—to identify systematic strengths and weaknesses. We further investigate model interpretability using explainability methods like Grad-CAM to visualize which parts of an image the model focuses on when generating a caption. This detailed analysis is complemented by a mandatory diary documenting the process of identifying and resolving common bugs in LLM-generated code, a key learning objective of this work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffee9cda",
   "metadata": {},
   "source": [
    "# Code and Directory Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9d37c3",
   "metadata": {},
   "source": [
    "The assignment is organized into a modular directory structure for clarity and ease of development. This Jupyter Notebook serves as the submission report with all the code arranged in code cells. For actual execution purposes, refer to the [GitHub repository](https://github.com/Kshitij-Vaidya/EE782-Assignments):\n",
    "```text\n",
    "imageCaptioning/\n",
    "│\n",
    "├── config.py                # Configuration settings and logger setup\n",
    "├── main.py                  # Entry point for training and evaluation\n",
    "│\n",
    "├── models/                  # Contains model definitions\n",
    "│   ├── encoder.py           # CNN encoder (ResNet-18, MobileNet)\n",
    "│   ├── lstmDecoder.py       # LSTM-based decoder\n",
    "│   ├── transformerDecoder.py# Transformer-based decoder\n",
    "│   └── captioner.py         # Wrapper combining encoder and decoder\n",
    "│\n",
    "├── data/                    # Data handling and preprocessing\n",
    "│   ├── dataset.py           # Custom dataset class for RSICD\n",
    "│   ├── preprocess.py        # Preprocessing utilities\n",
    "│   ├── prepareFromCSV.py    # Script to prepare images and annotations from CSV\n",
    "│   └── vocabulary.py        # Vocabulary/tokenizer management\n",
    "│\n",
    "├── evaluation/              # Evaluation scripts and metrics\n",
    "│   ├── metrics.py           # BLEU, METEOR, and other metrics\n",
    "│   ├── decoding.py          # Decoding utilities for inference\n",
    "│   └── getCaptions.py       # Caption extraction and formatting\n",
    "│\n",
    "├── training/                # Training utilities\n",
    "│   ├── train.py             # Training loop and logic\n",
    "│   ├── lossFunction.py      # Loss functions\n",
    "│   ├── optimizer.py         # Optimizer setup\n",
    "│   └── utils.py             # Helper functions\n",
    "│\n",
    "├── outputs/                 # Stores model outputs, predictions, and statistics\n",
    "├── checkpoints/             # Saved model weights\n",
    "└── rsicdDataset/            # Dataset files and images (train, valid, test splits)\n",
    "```\n",
    "\n",
    "This structure separates data processing, model definition, training, and evaluation, making the codebase easy to navigate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76d30c3",
   "metadata": {},
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3b88c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imageCaptioning/config.py\n",
    "\n",
    "import os\n",
    "import logging\n",
    "import re\n",
    "import psutil\n",
    "from typing import List\n",
    "\n",
    "# Define the Logging Configuration\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"[%(asctime)s] %(levelname)s:%(name)s: %(message)s\",\n",
    "    datefmt=\"%H:%M:%S\"\n",
    ")\n",
    "LOGGER = logging.getLogger(\"Preprocessing\")\n",
    "\n",
    "def getCustomLogger(name : str) -> logging.Logger:\n",
    "    return logging.getLogger(name)\n",
    "\n",
    "# Data Location Configuration\n",
    "PROJECT_ROOT = os.path.dirname(os.path.abspath(__file__))\n",
    "DATA_ROOT = os.path.join(PROJECT_ROOT, \"rsicdDataset\")\n",
    "OUTPUT_DIRECTORY = os.path.join(PROJECT_ROOT, \"outputs\")\n",
    "CHECKPOINT_PATH = os.path.join(PROJECT_ROOT, \"checkpoints\")\n",
    "\n",
    "# Define the common tokenize to be used across the project\n",
    "def tokenize(text : str) -> List[str]:\n",
    "    return re.findall(r'\\w+', text.lower())\n",
    "\n",
    "# Device and Utility Details for the Torch Modules and Training\n",
    "DEVICE = \"cpu\"\n",
    "\n",
    "def getMemoryMB():\n",
    "    process = psutil.Process(os.getpid())\n",
    "    return process.memory_info().rss / (1024 ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ed5c8d",
   "metadata": {},
   "source": [
    "## Data Capture and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67b6fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imageCaptioning/data/prepareFromCSV.py\n",
    "\n",
    "import csv\n",
    "import sys\n",
    "csv.field_size_limit(sys.maxsize)\n",
    "import ast\n",
    "import os\n",
    "import json\n",
    "import argparse\n",
    "import re\n",
    "\n",
    "from imageCaptioning.config import DATA_ROOT, LOGGER\n",
    "\n",
    "def prepareFromCSV(csvPath : str, split : str, outputDirectory : str = DATA_ROOT):\n",
    "    \"\"\"\n",
    "    Convert CSV Data with Bytes Info into:\n",
    "        1. Images / Directory of JPG Files\n",
    "        2. annotations.json with the metadata for the preprocessing\n",
    "    \"\"\"\n",
    "    imageDirectory = os.path.join(outputDirectory, \"images\", split)\n",
    "    os.makedirs(imageDirectory, exist_ok=True)\n",
    "\n",
    "    annotations = []\n",
    "    LOGGER.info(f\"Reading RSICD CSV from {csvPath} ...\")\n",
    "\n",
    "    with open(csvPath, \"r\", encoding=\"utf-8\") as file:\n",
    "        reader = csv.reader(file)\n",
    "        for index, row in enumerate(reader):\n",
    "            if not row or len(row) < 3:\n",
    "                continue\n",
    "            filename = row[0]\n",
    "            captionString = row[1]\n",
    "            byteString = row[2]\n",
    "            # Parse the captions\n",
    "            try:\n",
    "                captionsList = ast.literal_eval(captionString)\n",
    "                captionsList = [str(c).strip() for c in captionsList]\n",
    "\n",
    "                if len(captionsList) == 1:\n",
    "                    joined = captionsList[0]\n",
    "                    joined = re.sub(r'\\.(\\w)', r'. \\1', joined)\n",
    "                    splitCaptions = re.split(r'\\.\\s+', joined)\n",
    "                    captionsList = [c.strip() for c in splitCaptions if c.strip()]\n",
    "            except Exception:\n",
    "                captionsList = captionString.strip(\"[]\").replace(\"'\", \"\").split(\",\")\n",
    "                captionsList = [c.strip() for c in captionsList if c.strip()]\n",
    "\n",
    "            # Parse the Image Bytes\n",
    "            try:\n",
    "                dictObject = ast.literal_eval(byteString)\n",
    "                imageBytes = dictObject[\"bytes\"]\n",
    "            except Exception as e:\n",
    "                LOGGER.warning(f\"Row {index}: Failed to parse image bytes ({e})\")\n",
    "                continue\n",
    "\n",
    "            # Save the image\n",
    "            basename = os.path.basename(filename)\n",
    "            outputPath = os.path.join(imageDirectory, basename)\n",
    "            with open(outputPath, \"wb\") as image:\n",
    "                image.write(imageBytes)\n",
    "\n",
    "            # Collect the metadata\n",
    "            annotations.append({\n",
    "                \"imageId\" : os.path.splitext(basename)[0],\n",
    "                \"filename\" : basename,\n",
    "                \"captions\" : captionsList,\n",
    "            })\n",
    "        \n",
    "        # Save annotations to the json file\n",
    "        annotationPath = os.path.join(outputDirectory, f\"{split}Annotations.json\")\n",
    "        with open(annotationPath, \"w\") as jsonFile:\n",
    "            json.dump(annotations, jsonFile, indent=2)\n",
    "        \n",
    "        LOGGER.info(f\"Saved {len(annotations)} images : {imageDirectory}\")\n",
    "        LOGGER.info(f\"Saved Metadata : {annotationPath}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description=\"Prepare the RSICD dataset from the CSV file\")\n",
    "    parser.add_argument(\"--csv\",\n",
    "                        type=str,\n",
    "                        required=True,\n",
    "                        help=\"Path to the RSICD CSV files\")\n",
    "    parser.add_argument(\"--output\",\n",
    "                        type=str,\n",
    "                        default=DATA_ROOT,\n",
    "                        help=\"Output Directory (default : DATA_ROOT)\")\n",
    "    parser.add_argument(\"--split\",\n",
    "                        type=str,\n",
    "                        choices=[\"train\", \"test\", \"valid\"],\n",
    "                        required=True,\n",
    "                        help=\"Split type of the image data\")\n",
    "    arguments = parser.parse_args()\n",
    "\n",
    "    prepareFromCSV(arguments.csv, arguments.split, arguments.output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18aac09",
   "metadata": {},
   "source": [
    "```text\n",
    "SAVING TRAIN IMAGES\n",
    "\n",
    "(3.11.0) (base) kshitijvaidya@Kshitijs-MacBook-Pro-2 data % python prepareFromCSV.py --csv /Users/kshitijvaidya/EE782-AdvancedTopicsInML/Assignment1/imageCaptioning/rsicdDataset/train.csv --output ../rsicdDataset --split train\n",
    "[23:50:38] INFO:DataPrepare: Reading RSICD CSV from /Users/kshitijvaidya/EE782-AdvancedTopicsInML/Assignment1/imageCaptioning/rsicdDataset/train.csv ...\n",
    "[23:50:38] WARNING:DataPrepare: Row 0: Failed to parse image bytes (malformed node or string on line 1: <ast.Name object at 0x101144c10>)\n",
    "[23:50:59] INFO:DataPrepare: Saved 8734 images : ../rsicdDataset/images/train\n",
    "[23:50:59] INFO:DataPrepare: Saved Metadata : ../rsicdDataset/trainAnnotations.json\n",
    "\n",
    "SAVING TEST IMAGES\n",
    "\n",
    "(3.11.0) (base) kshitijvaidya@Kshitijs-MacBook-Pro-2 data % python prepareFromCSV.py --csv /Users/kshitijvaidya/EE782-AdvancedTopicsInML/Assignment1/imageCaptioning/rsicdDataset/test.csv --output ../rsicdDataset --split test \n",
    "[23:52:01] INFO:DataPrepare: Reading RSICD CSV from /Users/kshitijvaidya/EE782-AdvancedTopicsInML/Assignment1/imageCaptioning/rsicdDataset/test.csv ...\n",
    "[23:52:01] WARNING:DataPrepare: Row 0: Failed to parse image bytes (malformed node or string on line 1: <ast.Name object at 0x100c68c10>)\n",
    "[23:52:04] INFO:DataPrepare: Saved 1093 images : ../rsicdDataset/images/test\n",
    "[23:52:04] INFO:DataPrepare: Saved Metadata : ../rsicdDataset/testAnnotations.json\n",
    "\n",
    "SAVING VALID IMAGES\n",
    "\n",
    "(3.11.0) (base) kshitijvaidya@Kshitijs-MacBook-Pro-2 data % python prepareFromCSV.py --csv /Users/kshitijvaidya/EE782-AdvancedTopicsInML/Assignment1/imageCaptioning/rsicdDataset/valid.csv --output ../rsicdDataset --split valid \n",
    "[23:52:43] INFO:DataPrepare: Reading RSICD CSV from /Users/kshitijvaidya/EE782-AdvancedTopicsInML/Assignment1/imageCaptioning/rsicdDataset/valid.csv ...\n",
    "[23:52:43] WARNING:DataPrepare: Row 0: Failed to parse image bytes (malformed node or string on line 1: <ast.Name object at 0x10496cc10>)\n",
    "[23:52:46] INFO:DataPrepare: Saved 1094 images : ../rsicdDataset/images/valid\n",
    "[23:52:46] INFO:DataPrepare: Saved Metadata : ../rsicdDataset/validAnnotations.json\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8d3e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imageCaptioning/data/dataset.py\n",
    "\n",
    "import os\n",
    "import json\n",
    "from typing import Optional, List\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "\n",
    "from imageCaptioning.data.vocabulary import Vocabulary\n",
    "\n",
    "class RSICDDataset(Dataset):\n",
    "    def __init__(self, root : str, \n",
    "                 split : str,\n",
    "                 vocab: Optional[\"Vocabulary\"] = None,\n",
    "                 transform: Optional[transforms.Compose] = None,\n",
    "                 maxLength : int = 24):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            1. root (str): dataset root containing the images/{split}/ and annotations.json file\n",
    "            2. split (str): train | test | valid\n",
    "            3. vocab (Vocabulary): tokenizer/vocab object only needed for caption encoding\n",
    "            4. transform : torchvision transforms\n",
    "            5. maxLength (int): max caption length\n",
    "        \"\"\"\n",
    "        self.split = split\n",
    "        self.root = root\n",
    "        self.vocabulary = vocab\n",
    "        self.maxLength = maxLength\n",
    "        self.transform = transform\n",
    "\n",
    "        annotationPath = os.path.join(root, f\"{self.split}Annotations.json\")\n",
    "        with open(annotationPath, \"r\") as file:\n",
    "            self.annotations = json.load(file)\n",
    "        \n",
    "        self.imageDirectory = os.path.join(root, \"images\", self.split)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        annotation = self.annotations[index]\n",
    "        imagePath = os.path.join(self.imageDirectory, annotation[\"filename\"])\n",
    "        image = Image.open(imagePath).convert(\"RGB\")\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        # Use the first caption\n",
    "        caption = annotation[\"captions\"][0]\n",
    "        if self.vocabulary:\n",
    "            tokenIds, _ = self.vocabulary.encode(caption, maxLength = self.maxLength)\n",
    "            tokens = torch.tensor(tokenIds, dtype = torch.long)\n",
    "        else:\n",
    "            tokens = caption\n",
    "        return image, tokens\n",
    "    \n",
    "    def getImagePaths(self) -> List[str]:\n",
    "        \"\"\"\n",
    "        Returns list of all image file paths in the dataset\n",
    "        \"\"\"\n",
    "        return [os.path.join(self.imageDirectory, annotation[\"filename\"])\n",
    "                for annotation in self.annotations]\n",
    "    \n",
    "    def loadImage(self, imagePath: Path) -> Image:\n",
    "        \"\"\"\n",
    "        Loads and transforms an image from the given Path object\n",
    "        \"\"\"\n",
    "        image = Image.open(str(imagePath)).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ba8744",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imageCaptioning/data/vocabulary.py\n",
    "\n",
    "from typing import Dict, List, Tuple\n",
    "import json\n",
    "from collections import Counter\n",
    "\n",
    "from imageCaptioning.config import LOGGER\n",
    "\n",
    "\n",
    "class Vocabulary:\n",
    "    \"\"\"\n",
    "    Word Level Vocabulary for image captioning\n",
    "    Handles token-to-index and index-to-token mappings\n",
    "    \"\"\"\n",
    "    def __init__(self, counter : Counter,\n",
    "                 minFrequency: int = 1,\n",
    "                 maxSize: int = 10000) -> None:\n",
    "        self.frequencies: Counter = counter\n",
    "        self.minFrequency: int = minFrequency\n",
    "        self.maxSize: int = maxSize\n",
    "\n",
    "        # Special Tokens\n",
    "        self.padToken: str = \"<pad>\"\n",
    "        self.bosToken: str = \"<bos>\"\n",
    "        self.eosToken: str = \"<eos>\"\n",
    "\n",
    "        # Define the string to int and int to string mappings\n",
    "        self.STOI: Dict[str, int] = {}\n",
    "        self.ITOS: Dict[int, str] = {}\n",
    "\n",
    "        # Build the Vocabulary on Initialization\n",
    "        self._buildVocabulary()\n",
    "    \n",
    "    def _buildVocabulary(self) -> None:\n",
    "        tokens = [self.padToken, self.bosToken, self.eosToken]\n",
    "        mostCommon = [word for word, count in self.frequencies.items() if count >= self.minFrequency]\n",
    "        mostCommon = mostCommon[: self.maxSize - len(tokens)]\n",
    "        tokens.extend(mostCommon)\n",
    "\n",
    "        self.STOI = {token : index for index, token in enumerate(tokens)}\n",
    "        self.ITOS = {index : token for token, index in self.STOI.items()}\n",
    "\n",
    "        LOGGER.info(\n",
    "            f\"Built Vocabulary: Size={len(self)}, \"\n",
    "            f\"Minimum Frequency={self.minFrequency}, \"\n",
    "            f\"Maximum Size={self.maxSize}, \"\n",
    "            f\"Unique Tokens={len(self.frequencies)}\"\n",
    "        )\n",
    "        \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.STOI)\n",
    "\n",
    "    def encode(self, text : str, maxLength: int = 24) -> Tuple[List[int], int]:\n",
    "        \"\"\"\n",
    "        Convert the caption string into a list of token IDs with BOS/EOS and padding\n",
    "        \"\"\"\n",
    "        tokens = text.lower().split()\n",
    "        rawTokenLength = len(tokens)\n",
    "        tokenIds = [self.STOI.get(self.bosToken)]\n",
    "        tokenIds += [self.STOI.get(token, self.STOI.get(self.padToken)) for token in tokens]\n",
    "        tokenIds.append(self.STOI.get(self.eosToken))\n",
    "        # Padding / Truncation\n",
    "        if (len(tokenIds)) < maxLength:\n",
    "            tokenIds.extend([self.STOI.get(self.padToken)] * (maxLength - len(tokenIds)))\n",
    "        else:\n",
    "            tokenIds = tokenIds[:maxLength]\n",
    "        return tokenIds, rawTokenLength\n",
    "    \n",
    "    def decode(self, tokenIds: List[int]) -> str:\n",
    "        \"\"\"\n",
    "        Convert list of token IDs to the caption of strings stopping at EOS\n",
    "        \"\"\"\n",
    "        captionWords = []\n",
    "        for index in tokenIds:\n",
    "            token = self.ITOS.get(index, self.padToken)\n",
    "            if (token == self.eosToken):\n",
    "                break\n",
    "            if token not in {self.padToken, self.bosToken}:\n",
    "                captionWords.append(token)\n",
    "        return \" \".join(captionWords)\n",
    "\n",
    "    def save(self, path : str) -> None:\n",
    "        object = {\n",
    "            \"STOI\" : self.STOI,\n",
    "            \"ITOS\" : self.ITOS,\n",
    "            \"Pad\" : self.STOI.get(self.padToken, 0),\n",
    "            \"BOS\" : self.STOI.get(self.bosToken, 1),\n",
    "            \"EOS\" : self.STOI.get(self.eosToken, 2),\n",
    "            \"Size\" : len(self),\n",
    "            \"MinFrequency\" : self.minFrequency,\n",
    "            \"MaxSize\" : self.maxSize\n",
    "        }\n",
    "        with open(path, \"w\") as file:\n",
    "            json.dump(obj=object, fp=file, indent=2)\n",
    "    \n",
    "    @classmethod\n",
    "    def load(cls, path : str) -> \"Vocabulary\":\n",
    "        with open(path, \"r\") as file:\n",
    "            object: Dict[str, int] = json.load(file)\n",
    "        vocabulary = cls.__new__(cls)\n",
    "        vocabulary.minFrequency = object.get(\"MinFrequency\", 1)\n",
    "        vocabulary.maxSize = object.get(\"MaxSize\", 10000)\n",
    "        vocabulary.padToken = \"<pad>\"\n",
    "        vocabulary.eosToken = \"<eos>\"\n",
    "        vocabulary.bosToken = \"<bos>\"\n",
    "        vocabulary.STOI = object.get(\"STOI\")\n",
    "        vocabulary.ITOS = object.get(\"ITOS\")\n",
    "        vocabulary.frequencies = Counter()\n",
    "        return vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a165cfdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imageCaptioning/data/preprocess.py\n",
    "\n",
    "import os\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "from torchvision import transforms\n",
    "\n",
    "from imageCaptioning.data.dataset import RSICDDataset\n",
    "from imageCaptioning.data.vocabulary import Vocabulary\n",
    "from imageCaptioning.config import (DATA_ROOT, LOGGER, \n",
    "                                    OUTPUT_DIRECTORY, tokenize)\n",
    "\n",
    "\n",
    "\n",
    "def getTransforms():\n",
    "    return transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        # Normalise according to the ImageNet Statistics\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                             std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "def buildVocabFromTrain(minFrequency: int = 3,\n",
    "                        maxSize: int = 10000) -> Vocabulary:\n",
    "    trainDataset = RSICDDataset(root=DATA_ROOT, split=\"train\")\n",
    "    counter = Counter()\n",
    "    LOGGER.debug(f\"Sample annotation : {trainDataset.annotations[0]['captions']}\")\n",
    "    for annotation in trainDataset.annotations:\n",
    "        for caption in annotation[\"captions\"]:\n",
    "            counter.update(tokenize(caption))\n",
    "    \n",
    "    for i, (word, freq) in enumerate(counter.most_common(20)):\n",
    "        LOGGER.debug(f\"Word {i}: {word}, freq={freq}\")\n",
    "    LOGGER.debug(f\"Total Unique tokens in counter: {len(counter)}\")\n",
    "    \n",
    "    vocabulary = Vocabulary(counter=counter,\n",
    "                            minFrequency=minFrequency,\n",
    "                            maxSize=maxSize)\n",
    "    vocabularyPath = os.path.join(OUTPUT_DIRECTORY, \"vocab.json\")\n",
    "    vocabulary.save(vocabularyPath)\n",
    "    LOGGER.info(f\"Saved vocabulary to {vocabularyPath} (Size = {len(vocabulary)})\")\n",
    "    return vocabulary\n",
    "\n",
    "def computeStatistics(vocabulary: Vocabulary,\n",
    "                      split: str,\n",
    "                      maxLength: int = 24) -> None:\n",
    "    dataset = RSICDDataset(root=DATA_ROOT,\n",
    "                           split=split,\n",
    "                           vocab=vocabulary,\n",
    "                           transform=getTransforms(),\n",
    "                           maxLength=maxLength)\n",
    "    lengths = []\n",
    "    OOVCount, totalCount = 0, 0\n",
    "\n",
    "    for annotation in dataset.annotations:\n",
    "        for caption in annotation[\"captions\"]:\n",
    "            tokens = tokenize(caption)\n",
    "            _, rawLength = vocabulary.encode(caption, maxLength=maxLength)\n",
    "            lengths.append(rawLength)\n",
    "            totalCount += len(tokens)\n",
    "            OOVCount += sum(1 for word in tokens if word not in vocabulary.STOI)\n",
    "    \n",
    "    coverage = 100 * (1 - OOVCount / totalCount)\n",
    "    LOGGER.info(f\"[{split}] Vocabulary Coverage : {coverage:.2f}\")\n",
    "\n",
    "    # Histogram Plot\n",
    "    plt.hist(lengths, bins=20)\n",
    "    plt.title(f\"{split} Caption Length Distribution\")\n",
    "    plt.savefig(os.path.join(OUTPUT_DIRECTORY, f\"{split}LengthsHistogram.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    return {\n",
    "        \"split\" : split,\n",
    "        \"coverage\" : coverage,\n",
    "        \"averageLength\" : np.mean(lengths),\n",
    "        \"standardDeviation\" : np.std(lengths),\n",
    "    }\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    LOGGER.info(\"Building Vocabulary from Training Annotations\")\n",
    "    vocabulary = buildVocabFromTrain()\n",
    "    LOGGER.info(\"Computing training and validation statistics\")\n",
    "    trainingStatistics = computeStatistics(vocabulary=vocabulary, split=\"train\")\n",
    "    validationStatistics = computeStatistics(vocabulary=vocabulary, split=\"valid\")\n",
    "    data = pd.DataFrame([trainingStatistics, validationStatistics])\n",
    "    data.to_csv(os.path.join(OUTPUT_DIRECTORY, \"tokenStatisticsTrainingValidation.csv\"), index=False)\n",
    "    LOGGER.info(\"Saved token statistics CSV\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb0d93e",
   "metadata": {},
   "source": [
    "```text\n",
    "RUNNING PREPROCESS TO BUILD VOCAB.json\n",
    "\n",
    "(.venv) kshitijvaidya@Kshitijs-MacBook-Pro-2 Assignment1 % python -m imageCaptioning.data.preprocess\n",
    "[14:40:18] INFO:Preprocessing: Building Vocabulary from Training Annotations\n",
    "[14:40:18] INFO:Preprocessing: Built Vocabulary: Size=2701, Minimum Frequency=1, Maximum Size=10000, Unique Tokens=2698\n",
    "[14:40:18] INFO:Preprocessing: Saved vocabulary to ./imageCaptioning/outputs/vocab.json (Size = 2701)\n",
    "[14:40:18] INFO:Preprocessing: Computing training and validation statistics\n",
    "[14:40:18] INFO:Preprocessing: [train] Vocabulary Coverage : 100.00\n",
    "[14:40:19] INFO:Preprocessing: [valid] Vocabulary Coverage : 99.04\n",
    "[14:40:19] INFO:Preprocessing: Saved token statistics CSV\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4b1665",
   "metadata": {},
   "source": [
    "## Model Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213e860a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imageCaptioning/models/encoder.py\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from typing import List, Dict\n",
    "from imageCaptioning.config import getCustomLogger, DEVICE, OUTPUT_DIRECTORY\n",
    "from imageCaptioning.data.dataset import RSICDDataset\n",
    "\n",
    "LOGGER = getCustomLogger(\"Encoder\")\n",
    "\n",
    "class CNNEncoder(nn.Module):\n",
    "    '''\n",
    "    CNN Encoder supporting ResNet-18 and MobileNet\n",
    "    Resolves  classifier head, applies global average pooling\n",
    "    '''\n",
    "    def __init__(self, modelName: str = 'resnet18',\n",
    "                 pretrained: bool = True,\n",
    "                 finetune: bool = True,\n",
    "                 numLayers: int = 2,\n",
    "                 outputDim: int = 512) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        if modelName == 'resnet18':\n",
    "            baseModel = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1 if pretrained else None)\n",
    "            modules = list(baseModel.children())[:-2]\n",
    "            self.CNN = nn.Sequential(*modules)\n",
    "            featureDim = baseModel.fc.in_features\n",
    "        \n",
    "        elif modelName == \"mobilenet\":\n",
    "            baseModel = models.mobilenet_v2(weights=models.MobileNet_V2_Weights.IMAGENET1K_V1 if pretrained else None)\n",
    "            self.CNN = baseModel.features\n",
    "            featureDim = baseModel.last_channel\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported Model Name: {modelName}\")\n",
    "        \n",
    "        # Global Average Pooling\n",
    "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        # Projection into a Common Dimension\n",
    "        self.projection = nn.Linear(featureDim, outputDim)\n",
    "        # Define the finetuning policy\n",
    "        parameters = list(self.CNN.children())\n",
    "        for layer in parameters[-numLayers:]:\n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = finetune\n",
    "        LOGGER.info(f\"Initialized CNN Encoder with {modelName},\"\n",
    "                    f\"Finetune = {finetune}, Output Dimensions = {outputDim}\")\n",
    "    \n",
    "\n",
    "    def forward(self, x : torch.Tensor) -> torch.Tensor:\n",
    "        features = self.CNN(x)\n",
    "        pooledOutput = self.pool(features).squeeze(-1).squeeze(-1)\n",
    "        return self.projection(pooledOutput)\n",
    "    \n",
    "    def cacheFeatures(self, imagePaths : List[str],\n",
    "                      dataset: RSICDDataset,\n",
    "                      batchSize: int = 32,\n",
    "                      savePath: str = \"featureCache.pt\") -> None:\n",
    "        self.eval()\n",
    "        featureDict = {}\n",
    "        with torch.no_grad():\n",
    "            for i in range(0, len(imagePaths), batchSize):\n",
    "                batchPaths = imagePaths[i : i + batchSize]\n",
    "                batchImages = [dataset.loadImage(path).to(DEVICE)\n",
    "                               for path in batchPaths]\n",
    "                batchTensors = torch.stack(batchImages)\n",
    "                batchFeatures = self.forward(batchTensors).cpu()\n",
    "\n",
    "                for imagePath, features in zip(batchPaths, batchFeatures):\n",
    "                    featureDict[imagePath] = features\n",
    "        savePath = os.path.join(OUTPUT_DIRECTORY, savePath)\n",
    "        torch.save(featureDict, savePath)\n",
    "        LOGGER.info(f\"Cached features for {len(featureDict)} images to {savePath}\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def loadCachedFeatures(loadPath) -> Dict:\n",
    "        \"\"\"\n",
    "        Load the cached features from a .pt file\n",
    "        \"\"\"\n",
    "        return torch.load(loadPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898e4054",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imageCaptioning/models/lstmDecoder.py\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import List\n",
    "from imageCaptioning.config import getCustomLogger, DEVICE\n",
    "\n",
    "LOGGER = getCustomLogger(\"LSTM Decoder\")\n",
    "\n",
    "class LSTMDecoder(nn.Module):\n",
    "    '''\n",
    "    LSTM Caption Decoder\n",
    "    Projects CNN feature to init hidden feature\n",
    "    '''\n",
    "    def __init__(self,\n",
    "                 vocabSize: int,\n",
    "                 embedDimension: int = 256,\n",
    "                 hiddenDimension: int = 512,\n",
    "                 numLayers: int = 2,\n",
    "                 paddingIndex: int = 0,\n",
    "                 dropout: float = 0.5) -> None:\n",
    "        super().__init__()\n",
    "        self.numLayers = numLayers\n",
    "        self.embedding = nn.Embedding(vocabSize, \n",
    "                                      embedDimension, \n",
    "                                      padding_idx=paddingIndex)\n",
    "        self.LSTM = nn.LSTM(embedDimension, \n",
    "                            hiddenDimension,\n",
    "                            numLayers,\n",
    "                            dropout=dropout,\n",
    "                            batch_first=True)\n",
    "        self.fc = nn.Linear(hiddenDimension, vocabSize)\n",
    "\n",
    "        self.initH = nn.Linear(hiddenDimension, hiddenDimension)\n",
    "        self.initC = nn.Linear(hiddenDimension, hiddenDimension)\n",
    "\n",
    "        LOGGER.info(f\"Initialised LSTM Decoder with Vocabulary={vocabSize}, \"\n",
    "                    f\"Embedding Dimension = {embedDimension}, Hidden Dimensions = {hiddenDimension}, Layers = {numLayers}\")\n",
    "    \n",
    "    def forward(self, features: torch.Tensor,\n",
    "                captions: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Teacher Forcing Mode\n",
    "        Arguments:\n",
    "            features: (B, hiddenDimension) : initialised hidden tokens\n",
    "            captions: (B, L): input sequence of tokens\n",
    "        \"\"\"\n",
    "        embeddings = self.embedding(captions)\n",
    "        H0 = torch.tanh(self.initH(features)).unsqueeze(0).repeat(self.numLayers, 1, 1) # (numLayers, B, H)\n",
    "        C0 = torch.tanh(self.initC(features)).unsqueeze(0).repeat(self.numLayers, 1, 1) # (numLayers, B, H)\n",
    "\n",
    "        outputs, _ = self.LSTM(embeddings, (H0, C0))\n",
    "        return self.fc(outputs) # Output Size : (B, L, vocabSize)\n",
    "\n",
    "    def generate(self, features: torch.Tensor,\n",
    "                 maxLength: int = 24,\n",
    "                 BOSIndex: int = 1,\n",
    "                 EOSIndex: int = 2) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Greedy Decoding\n",
    "        \"\"\"\n",
    "        H = torch.tanh(self.initH(features)).unsqueeze(0).repeat(self.numLayers, 1, 1)\n",
    "        C = torch.tanh(self.initH(features)).unsqueeze(0).repeat(self.numLayers, 1, 1)\n",
    "        inputs = torch.Tensor([BOSIndex],\n",
    "                              device=features.device).unsqueeze(0)\n",
    "        embeddings = self.embedding(inputs)\n",
    "        outputs = []\n",
    "\n",
    "        for _ in range(maxLength):\n",
    "            output, (H, C) = self.LSTM(embeddings, (H, C))\n",
    "            logits = self.fc(output[:, -1, :]) # Last token \n",
    "            predicted = torch.argmax(logits, dim=-1)\n",
    "            outputs.append(predicted.item())\n",
    "            if predicted.item() == EOSIndex:\n",
    "                break\n",
    "            embeddings = self.embedding(predicted.unsqueeze(0))\n",
    "        \n",
    "        return outputs\n",
    "    \n",
    "    def generateBatch(self, features: torch.Tensor,\n",
    "                      maxLength: int = 24,\n",
    "                      BOSIndex: int = 1,\n",
    "                      EOSIndex: int = 2) -> List[List[int]]:\n",
    "        batchSize = features.size(0)\n",
    "        H = torch.tanh(self.initH(features)).unsqueeze(0).repeat(self.numLayers, 1, 1)\n",
    "        C = torch.tanh(self.initC(features)).unsqueeze(0).repeat(self.numLayers, 1, 1)\n",
    "        inputs = torch.full((batchSize, 1), BOSIndex,\n",
    "                            dtype=torch.long,\n",
    "                            device=DEVICE)\n",
    "        outputs = [[] for _ in range(batchSize)]\n",
    "        finished = torch.zeros(batchSize, dtype=torch.bool,\n",
    "                               device=DEVICE)\n",
    "        \n",
    "        for _ in range(maxLength):\n",
    "            embeddings = self.embedding(inputs)\n",
    "            output, (H, C) = self.LSTM(embeddings, (H, C))\n",
    "            logits = self.fc(output[:, -1, :])\n",
    "            predicted = torch.argmax(logits, dim=-1)\n",
    "\n",
    "            for i in range(batchSize):\n",
    "                if not finished[i]:\n",
    "                    outputs[i].append(predicted[i].item())\n",
    "                    if predicted[i].item() == EOSIndex:\n",
    "                        finished[i] = True\n",
    "            \n",
    "            if finished.all():\n",
    "                break\n",
    "\n",
    "            inputs = predicted.unsqueeze(1)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d369c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imageCaptioning/models/transformerDecoder.py\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import List\n",
    "\n",
    "from imageCaptioning.config import getCustomLogger, DEVICE\n",
    "\n",
    "LOGGER = getCustomLogger(\"Transformer Decoder\")\n",
    "\n",
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 vocabSize: int,\n",
    "                 dModel: int = 256,\n",
    "                 numLayers: int = 2,\n",
    "                 numHeads: int = 2,\n",
    "                 ffDim: int = 1024,\n",
    "                 dropout: float = 0.2,\n",
    "                 paddingIndex: int = 0,\n",
    "                 encoderDim: int = 512) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocabSize, dModel, padding_idx=paddingIndex)\n",
    "        self.positionEncoder = nn.Parameter(torch.zeros(1, 32, dModel)) # Learnable Positional Encoder\n",
    "        decoderLayer = nn.TransformerDecoderLayer(dModel, numHeads, ffDim, dropout=dropout, batch_first=True)\n",
    "        self.decoder = nn.TransformerDecoder(decoderLayer, numLayers)\n",
    "        self.fc = nn.Linear(dModel, vocabSize)\n",
    "        # Image Projection : Convert feature into memory tokens\n",
    "        self.imageProjection = nn.Linear(encoderDim, dModel)\n",
    "\n",
    "        LOGGER.info(f\"Initialised Transformer Decoder with Vocab = {vocabSize}, \"\n",
    "                    f\"d_model = {dModel}, Layers = {numLayers}, Heads = {numHeads}\")\n",
    "    \n",
    "    def forward(self, features: torch.Tensor,\n",
    "                captions: torch.Tensor) -> torch.Tensor:\n",
    "        _, L = captions.shape\n",
    "        embeddings = self.embedding(captions) + self.positionEncoder[:, :L, :]\n",
    "        # Project the features onto a (B, 1, D) Memory\n",
    "        memory = self.imageProjection(features).unsqueeze(1)\n",
    "        # Causal Mask for the Decoder\n",
    "        mask = torch.triu(torch.ones(L, L), diagonal=1).bool().to(captions.device)\n",
    "        output = self.decoder(tgt=embeddings,\n",
    "                              memory=memory,\n",
    "                              tgt_mask=mask)\n",
    "        return self.fc(output)\n",
    "    \n",
    "    def generateBatch(self, features : torch.Tensor,\n",
    "                      maxLength : int = 24,\n",
    "                      BOSIndex : int = 1,\n",
    "                      EOSIndex : int = 2) -> List[List[int]]:\n",
    "        batchSize = features.size(0)\n",
    "        generated = torch.full((batchSize, 1), \n",
    "                               BOSIndex, dtype=torch.long)\n",
    "        finished = torch.zeros(batchSize, dtype=torch.bool,\n",
    "                               device=DEVICE)\n",
    "        predictions : List[List[int]] = [[] for _ in range(batchSize)]\n",
    "\n",
    "        for _ in range(maxLength):\n",
    "            # Embeddings of Dimension : (B, seqLen, dModel)\n",
    "            embeddings = self.embedding(generated) + self.positionEncoder[:, :generated.size(1), :]\n",
    "            memory = self.imageProjection(features).unsqueeze(1)\n",
    "            mask = torch.triu(torch.ones(generated.size(1), generated.size(1), device=DEVICE), diagonal=1).bool()\n",
    "            output = self.decoder(tgt=embeddings,\n",
    "                                  memory=memory,\n",
    "                                  tgt_mask=mask)\n",
    "            logits = self.fc(output[:, -1, :])\n",
    "            nextTokens = torch.argmax(logits, dim=1)\n",
    "\n",
    "            for i in range(batchSize):\n",
    "                if not finished[i]:\n",
    "                    predictions[i].append(nextTokens[i].item())\n",
    "                    if nextTokens[i].item() == EOSIndex:\n",
    "                        finished[i] = True\n",
    "            \n",
    "            if finished.all():\n",
    "                break\n",
    "\n",
    "            generated = torch.cat([generated, nextTokens.unsqueeze(1)], dim=1)\n",
    "        \n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221da464",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imageCaptioning/models/captioner.py\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from imageCaptioning.models.encoder import CNNEncoder\n",
    "from imageCaptioning.models.lstmDecoder import LSTMDecoder\n",
    "from imageCaptioning.models.transformerDecoder import TransformerDecoder\n",
    "from imageCaptioning.config import getCustomLogger\n",
    "\n",
    "LOGGER = getCustomLogger(\"Captioner\")\n",
    "\n",
    "class Captioner(nn.Module):\n",
    "    \"\"\"\n",
    "    Wrapper for the Encoder + Decoder Framework\n",
    "    \"\"\"\n",
    "    def __init__(self, vocabSize: int,\n",
    "                 modelType: str = \"lstm\",\n",
    "                 encoderName: str = \"resnet18\",\n",
    "                 finetune: bool = True) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = CNNEncoder(modelName=encoderName,\n",
    "                                  pretrained=True,\n",
    "                                  finetune=finetune,\n",
    "                                  outputDim=512)\n",
    "        \n",
    "        if modelType == \"lstm\":\n",
    "            self.decoder = LSTMDecoder(vocabSize=vocabSize)\n",
    "        elif modelType == \"transformer\":\n",
    "            self.decoder = TransformerDecoder(vocabSize=vocabSize)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown Decoder Type : {modelType}\")\n",
    "        \n",
    "        LOGGER.info(f\"Initialized Captioner with Encoder = {encoderName} and Decoder = {modelType}\")\n",
    "    \n",
    "    def forward(self, image: torch.Tensor,\n",
    "                captions: torch.Tensor) -> torch.Tensor:\n",
    "        features = self.encoder(image)\n",
    "        return self.decoder(features, captions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c208325",
   "metadata": {},
   "source": [
    "## Training Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace33c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imageCaptioning/training/utils.py\n",
    "\n",
    "from typing import Dict, Any\n",
    "import torch\n",
    "import random\n",
    "import numpy as np \n",
    "import os\n",
    "\n",
    "from imageCaptioning.config import getCustomLogger\n",
    "\n",
    "LOGGER = getCustomLogger(\"Utilities\")\n",
    "\n",
    "def getSeed(seed: int = 42) -> None:\n",
    "    \"\"\"\n",
    "    Ensure reproducibility across runs\n",
    "    \"\"\"\n",
    "    LOGGER.info(f\"Setting random seed = {seed}\")\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def saveCheckpoint(state : Dict[str, Any],\n",
    "                   filename : str) -> None:\n",
    "    \"\"\"\n",
    "    Save the model/optimizer to a file\n",
    "    \"\"\"\n",
    "    LOGGER.info(f\"Saving checkpoint to {filename}\")\n",
    "    os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "    torch.save(state, filename)\n",
    "\n",
    "def loadCheckpoint(filename : str,\n",
    "                   device: str = \"cpu\") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Load the model/optimizer\n",
    "    \"\"\"\n",
    "    LOGGER.info(f\"Loading checkpoint from {filename}\")\n",
    "    return torch.load(filename,\n",
    "                      map_location=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71cf2ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imageCaptioning/training/lossFunction.py\n",
    "\n",
    "import torch.nn as nn\n",
    "from imageCaptioning.config import getCustomLogger\n",
    "\n",
    "LOGGER = getCustomLogger(\"Loss Function\")\n",
    "\n",
    "def getLossFunction(paddingIndex: int) -> nn.Module:\n",
    "    \"\"\"\n",
    "    Return the Cross Entropy Loss ignoring the Padding Tokens\n",
    "    \"\"\"\n",
    "    LOGGER.info(f\"Initialised CrossEntropyLoss with ignore_index = {paddingIndex}\")\n",
    "    return nn.CrossEntropyLoss(ignore_index=paddingIndex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db58230",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imageCaptioning/training/optimizer.py\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim \n",
    "from torch.optim import Optimizer\n",
    "from torch.optim.lr_scheduler import _LRScheduler, StepLR\n",
    "\n",
    "from imageCaptioning.config import getCustomLogger\n",
    "\n",
    "LOGGER = getCustomLogger(\"Optimizer\")\n",
    "\n",
    "def buildOptimizer(model: nn.Module,\n",
    "                   lrCNN: float = 1e-4,\n",
    "                   lrDecoder: float = 2e-4,\n",
    "                   lrTransformer: float = 2e-5) -> Optimizer:\n",
    "    \"\"\"\n",
    "    Build the Adam Optimizer with separate CNN Encoder, LSYM/Transformer Decoder\n",
    "    \"\"\"\n",
    "    LOGGER.info(f\"Building Adam Optimizer (CNN LR = {lrCNN}, Decoder LR = {lrDecoder}, Transformer LR = {lrTransformer})\")\n",
    "    parameters = []\n",
    "\n",
    "    if hasattr(model, \"encoder\"):\n",
    "        parameters.append({\"params\" : model.encoder.parameters(),\n",
    "                           \"lr\" : lrCNN})\n",
    "    if hasattr(model, \"decoder\"):\n",
    "        parameters.append({\"params\" : model.decoder.parameters(),\n",
    "                           \"lr\" : lrDecoder})\n",
    "    if hasattr(model, \"transformer\"):\n",
    "        parameters.append({\"params\" : model.transformer.parameters(),\n",
    "                           \"lr\" : lrTransformer})\n",
    "    \n",
    "    optimizer = optim.Adam(params=parameters, betas=(0.9, 0.999))\n",
    "    return optimizer\n",
    "\n",
    "def buildScheduler(optimizer: Optimizer,\n",
    "                   stepSize: int = 5,\n",
    "                   gamma: float = 0.5) -> _LRScheduler:\n",
    "    \"\"\"\n",
    "    StepLR Scheduler: reduce the LR every 'stepSize' epochs by gamma\n",
    "    \"\"\"\n",
    "    LOGGER.info(f\"Using StepLR Scheduler: Step Size = {stepSize}, Gamma = {gamma}\")\n",
    "    return StepLR(optimizer=optimizer,\n",
    "                  step_size=stepSize,\n",
    "                  gamma=gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9a2ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imageCaptioning/training/train.py\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import time\n",
    "\n",
    "from imageCaptioning.training.lossFunction import getLossFunction\n",
    "from imageCaptioning.training.optimizer import buildOptimizer, buildScheduler\n",
    "from imageCaptioning.training.utils import saveCheckpoint\n",
    "from imageCaptioning.config import getCustomLogger, DEVICE, CHECKPOINT_PATH, getMemoryMB\n",
    "\n",
    "LOGGER = getCustomLogger(\"Train\")\n",
    "\n",
    "def trainEpoch(model: nn.Module,\n",
    "               dataloader: DataLoader,\n",
    "               criterion: nn.Module,\n",
    "               optimizer: torch.optim.Optimizer,\n",
    "               epoch: int,\n",
    "               gradClip: float = 5.0) -> float:\n",
    "    \"\"\"\n",
    "    Train model for a single epoch\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    totalLoss = 0.0\n",
    "\n",
    "    for _,(images, captions) in enumerate(tqdm(dataloader,\n",
    "                                               desc=f\"Epoch {epoch} [Train]\")):\n",
    "        images, captions = images.to(DEVICE), captions.to(DEVICE)\n",
    "\n",
    "        outputs : torch.Tensor = model(images, captions[:, :-1])\n",
    "        loss: torch.Tensor = criterion(outputs.reshape(-1, outputs.size(-1)),\n",
    "                                       captions[:, 1:].reshape(-1))\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), gradClip)\n",
    "        optimizer.step()\n",
    "\n",
    "        totalLoss += loss.item()\n",
    "    \n",
    "    return totalLoss / len(dataloader)\n",
    "\n",
    "\n",
    "def validate(model: nn.Module,\n",
    "             dataloader: DataLoader,\n",
    "             criterion: nn.Module,\n",
    "             epoch: int) -> float:\n",
    "    \"\"\"\n",
    "    Validate the model on the Validation Set\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    totalLoss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, captions in tqdm(dataloader,\n",
    "                                     desc=f\"Epoch {epoch} [Valid]\"):\n",
    "            images, captions = images.to(DEVICE), captions.to(DEVICE)\n",
    "            outputs: torch.Tensor = model(images, captions[:, :-1])\n",
    "            loss: torch.Tensor = criterion(outputs.reshape(-1, outputs.size(-1)),\n",
    "                                           captions[:, 1:].reshape(-1))\n",
    "            totalLoss += loss.item()\n",
    "    return totalLoss / len(dataloader)\n",
    "\n",
    "\n",
    "def trainModel(model: nn.Module,\n",
    "               trainLoader: DataLoader,\n",
    "               valLoader: DataLoader,\n",
    "               paddingIndex: int,\n",
    "               checkpointPath: str,\n",
    "               epochs: int = 50,\n",
    "               lrCNN: float = 1e-4,\n",
    "               lrDecoder: float = 2e-4,\n",
    "               lrTransformer: float = 2e-5) -> None:\n",
    "    \"\"\"\n",
    "    Main Training Loop\n",
    "    \"\"\"\n",
    "    criterion = getLossFunction(paddingIndex=paddingIndex)\n",
    "    optimizer = buildOptimizer(model, lrCNN, lrDecoder, lrTransformer)\n",
    "    scheduler = buildScheduler(optimizer)\n",
    "\n",
    "    bestValidationLoss = float(\"inf\")\n",
    "\n",
    "    startTime = time.time()\n",
    "    startMemory = getMemoryMB()\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        trainingLoss = trainEpoch(model, trainLoader, criterion, optimizer, epoch)\n",
    "        validationLoss = validate(model, valLoader, criterion, epoch)\n",
    "\n",
    "        LOGGER.info(f\"Epoch {epoch}: Train Loss = {trainingLoss:.4f}, Validation Loss = {validationLoss:.4f}\")\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        if validationLoss < bestValidationLoss:\n",
    "            LOGGER.info(f\"Validation Loss improved from {bestValidationLoss:.4f} to {validationLoss:.4f}\")\n",
    "            bestValidationLoss = validationLoss\n",
    "            saveCheckpoint({\n",
    "                \"Epoch\" : epoch,\n",
    "                \"modelState\" : model.state_dict(),\n",
    "                \"optimizerState\" : optimizer.state_dict(),\n",
    "                \"validationLoss\" : validationLoss\n",
    "            }, os.path.join(CHECKPOINT_PATH, checkpointPath))\n",
    "    \n",
    "    endTime = time.time()\n",
    "    endMemory = getMemoryMB()\n",
    "    LOGGER.info(f\"Training Time : {endTime - startTime:.2f} seconds\")\n",
    "    LOGGER.info(f\"Memory Usage Increase: {endMemory - startMemory:.2f}MB\")\n",
    "    LOGGER.info(f\"Peak Memory Usage at End: {endMemory:.2f}MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7c6109",
   "metadata": {},
   "source": [
    "## Main File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a830bc39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imageCaptioning/main.py\n",
    "\n",
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "import argparse\n",
    "import time\n",
    "# Import project modules\n",
    "from imageCaptioning.config import (DATA_ROOT, OUTPUT_DIRECTORY, DEVICE, CHECKPOINT_PATH,\n",
    "                                    getCustomLogger)\n",
    "from imageCaptioning.data.dataset import RSICDDataset\n",
    "from imageCaptioning.data.vocabulary import Vocabulary\n",
    "from imageCaptioning.data.preprocess import getTransforms\n",
    "from imageCaptioning.models.captioner import Captioner\n",
    "from imageCaptioning.training.train import trainModel\n",
    "\n",
    "LOGGER = getCustomLogger(\"Main\")\n",
    "\n",
    "def loadVocabulary(vocabPath: str) -> Vocabulary:\n",
    "    \"\"\"Load the preprocessed vocabulary\"\"\"\n",
    "    if not os.path.exists(vocabPath):\n",
    "        raise FileNotFoundError(f\"Vocabulary file not found at {vocabPath}\")\n",
    "    \n",
    "    LOGGER.info(f\"Loading vocabulary from {vocabPath}\")\n",
    "    vocabulary = Vocabulary.load(vocabPath)\n",
    "    LOGGER.info(f\"Loaded vocabulary with {len(vocabulary)} tokens\")\n",
    "    return vocabulary\n",
    "\n",
    "def createDataLoaders(vocabulary: Vocabulary, \n",
    "                     batchSize: int = 32,\n",
    "                     maxLength: int = 24,\n",
    "                     numWorkers: int = 4) -> tuple[DataLoader, DataLoader]:\n",
    "    \"\"\"Create training and validation data loaders\"\"\"\n",
    "    \n",
    "    # Define transforms\n",
    "    transform = getTransforms()\n",
    "    \n",
    "    # Create datasets\n",
    "    trainDataset = RSICDDataset(\n",
    "        root=DATA_ROOT,\n",
    "        split=\"train\",\n",
    "        vocab=vocabulary,\n",
    "        transform=transform,\n",
    "        maxLength=maxLength\n",
    "    )\n",
    "    \n",
    "    valDataset = RSICDDataset(\n",
    "        root=DATA_ROOT,\n",
    "        split=\"valid\",\n",
    "        vocab=vocabulary,\n",
    "        transform=transform,\n",
    "        maxLength=maxLength\n",
    "    )\n",
    "    \n",
    "    LOGGER.info(f\"Training dataset size: {len(trainDataset)}\")\n",
    "    LOGGER.info(f\"Validation dataset size: {len(valDataset)}\")\n",
    "    \n",
    "    # Create data loaders\n",
    "    trainLoader = DataLoader(\n",
    "        trainDataset,\n",
    "        batch_size=batchSize,\n",
    "        shuffle=True,\n",
    "        num_workers=numWorkers,\n",
    "        pin_memory=True if DEVICE == \"cuda\" else False\n",
    "    )\n",
    "    \n",
    "    valLoader = DataLoader(\n",
    "        valDataset,\n",
    "        batch_size=batchSize,\n",
    "        shuffle=False,\n",
    "        num_workers=numWorkers,\n",
    "        pin_memory=True if DEVICE == \"cuda\" else False\n",
    "    )\n",
    "    \n",
    "    return trainLoader, valLoader\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description=\"Train Image Captioning Model on RSICD Dataset\")\n",
    "    parser.add_argument(\"--batch-size\", type=int, default=32, help=\"Batch size for training\")\n",
    "    parser.add_argument(\"--epochs\", type=int, default=50, help=\"Number of training epochs\")\n",
    "    parser.add_argument(\"--lr-cnn\", type=float, default=1e-4, help=\"Learning rate for CNN encoder\")\n",
    "    parser.add_argument(\"--lr-decoder\", type=float, default=2e-4, help=\"Learning rate for decoder\")\n",
    "    parser.add_argument(\"--lr-transformer\", type=float, default=2e-5, help=\"Learning rate for transformer\")\n",
    "    parser.add_argument(\"--model-type\", type=str, default=\"lstm\", choices=[\"lstm\", \"transformer\"], \n",
    "                       help=\"Type of decoder to use\")\n",
    "    parser.add_argument(\"--encoder-name\", type=str, default=\"resnet18\", choices=[\"resnet18\", \"mobilenet\"],\n",
    "                       help=\"CNN encoder architecture\")\n",
    "    parser.add_argument(\"--max-length\", type=int, default=24, help=\"Maximum caption length\")\n",
    "    parser.add_argument(\"--finetune\", action=\"store_true\", help=\"Fine-tune the encoder\")\n",
    "    parser.add_argument(\"--num-workers\", type=int, default=4, help=\"Number of data loader workers\")\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    # Create checkpoint directory\n",
    "    os.makedirs(CHECKPOINT_PATH, exist_ok=True)\n",
    "    \n",
    "    LOGGER.info(\"Starting Image Captioning Training on RSICD Dataset\")\n",
    "    LOGGER.info(f\"Device: {DEVICE}\")\n",
    "    LOGGER.info(f\"Model Type: {args.model_type}\")\n",
    "    LOGGER.info(f\"Encoder: {args.encoder_name}\")\n",
    "    LOGGER.info(f\"Fine-tune Encoder: {args.finetune}\")\n",
    "    LOGGER.info(f\"Batch Size: {args.batch_size}\")\n",
    "    LOGGER.info(f\"Epochs: {args.epochs}\")\n",
    "    \n",
    "    # Load vocabulary\n",
    "    vocabPath = os.path.join(OUTPUT_DIRECTORY, \"vocab.json\")\n",
    "    vocabulary = loadVocabulary(vocabPath)\n",
    "    \n",
    "    # Create data loaders\n",
    "    trainLoader, valLoader = createDataLoaders(\n",
    "        vocabulary=vocabulary,\n",
    "        batchSize=args.batch_size,\n",
    "        maxLength=args.max_length,\n",
    "        numWorkers=args.num_workers\n",
    "    )\n",
    "    \n",
    "    # Initialize model\n",
    "    model = Captioner(\n",
    "        vocabSize=len(vocabulary),\n",
    "        modelType=args.model_type,\n",
    "        encoderName=args.encoder_name,\n",
    "        finetune=args.finetune\n",
    "    )\n",
    "    \n",
    "    # Move model to device\n",
    "    model = model.to(DEVICE)\n",
    "    \n",
    "    # Log model information\n",
    "    totalParams = sum(p.numel() for p in model.parameters())\n",
    "    trainableParams = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    LOGGER.info(f\"Total parameters: {totalParams:,}\")\n",
    "    LOGGER.info(f\"Trainable parameters: {trainableParams:,}\")\n",
    "    \n",
    "    # Define checkpoint path\n",
    "    checkpointPath = os.path.join(CHECKPOINT_PATH, f\"model_{args.model_type}_{args.encoder_name}.pt\")\n",
    "    \n",
    "    # Get padding index from vocabulary\n",
    "    paddingIndex = vocabulary.STOI.get(vocabulary.padToken, 0)\n",
    "    \n",
    "    # Train the model\n",
    "    LOGGER.info(\"Starting training...\")\n",
    "\n",
    "    trainModel(\n",
    "        model=model,\n",
    "        trainLoader=trainLoader,\n",
    "        valLoader=valLoader,\n",
    "        paddingIndex=paddingIndex,\n",
    "        epochs=args.epochs,\n",
    "        lrCNN=args.lr_cnn,\n",
    "        lrDecoder=args.lr_decoder,\n",
    "        lrTransformer=args.lr_transformer,\n",
    "        checkpointPath=checkpointPath\n",
    "    )\n",
    "    \n",
    "    LOGGER.info(\"Training completed!\")\n",
    "    LOGGER.info(f\"Best model saved to: {checkpointPath}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4fcfc6",
   "metadata": {},
   "source": [
    "# Results and Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcfed9a8",
   "metadata": {},
   "source": [
    "## Vocabulary Statistics\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "\n",
    "<div style=\"margin-bottom:32px\">\n",
    "    <img src=\"imageCaptioning/outputs/trainLengthsHistogram.png\" style=\"width:48%; display:inline-block;\"/>\n",
    "    <img src=\"imageCaptioning/outputs/validLengthsHistogram.png\" style=\"width:48%; display:inline-block;\"/>\n",
    "</div>\n",
    "\n",
    "<table style=\"margin: 0 auto;\">\n",
    "  <thead>\n",
    "    <tr>\n",
    "      <th>Split</th>\n",
    "      <th>Vocabulary Coverage (%)</th>\n",
    "      <th>OOV (%)</th>\n",
    "      <th>Average Caption Length</th>\n",
    "      <th>Std. Deviation</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <td>train</td>\n",
    "      <td>100.00</td>\n",
    "      <td>0.00</td>\n",
    "      <td>10.99</td>\n",
    "      <td>3.16</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>valid</td>\n",
    "      <td>99.04</td>\n",
    "      <td>0.96</td>\n",
    "      <td>9.28</td>\n",
    "      <td>3.00</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b210585",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
